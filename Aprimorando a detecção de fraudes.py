# -*- coding: utf-8 -*-
"""Desafio Deep Learning - Aprimorando a Detecção de Fraudes na QuantumFinance.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YwhL4h1aquqgJInyV6YO6T5KaFgoH1GX
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Carregando o conjunto de dados
data = pd.read_csv('/content/dataset.csv',delimiter=";")

# 1. Análise Exploratória de Dados
# Visualizando as primeiras linhas do conjunto de dados
print(data.head())

# Resumo estatístico das variáveis numéricas
print(data.describe())

# Verificando a distribuição das classes (Fraudulenta ou Legítima)
fraud_distribution = data['Fraudulent'].value_counts()
print("Distribuição de Fraudes:\n", fraud_distribution)

# Visualizando a distribuição da idade para transações Fraudulentas e Legítimas
plt.figure(figsize=(10, 6))
sns.histplot(data=data, x='Age', hue='Fraudulent', bins=30, kde=True)
plt.title('Distribuição de Idade para Transações Fraudulentas e Legítimas')
plt.xlabel('Idade')
plt.ylabel('Contagem')
plt.show()

# Explorando correlações entre variáveis numéricas
correlation_matrix = data.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matriz de Correlação')
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Carregando o conjunto de dados
data = pd.read_csv('/content/dataset.csv',delimiter=";")

# 2. Pré-processamento de Dados
# Removendo colunas não relevantes para o modelo
data = data.drop(['Transaction ID', 'Date', 'Time', 'Shipping Address', 'Billing Address'], axis=1)

# Codificando variáveis categóricas usando Label Encoding
label_encoder = LabelEncoder()
categorical_cols = ['Card Type', 'Entry Mode', 'Transaction Type', 'Merchant Group', 'Transaction Country', 'Gender', 'Issuing Bank']

for col in categorical_cols:
    data[col] = label_encoder.fit_transform(data[col])

# Normalizando as variáveis numéricas usando StandardScaler
numeric_cols = ['Amount', 'Age']
scaler = StandardScaler()

data[numeric_cols] = scaler.fit_transform(data[numeric_cols])

# Separando as features e o target
X = data.drop('Fraudulent', axis=1)
y = data['Fraudulent']

# Dividindo os dados em conjuntos de treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Exibindo as primeiras linhas dos dados após o pré-processamento
print(X_train.head())
print(y_train.head())

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Carregando o conjunto de dados
data = pd.read_csv('/content/dataset.csv',delimiter=";")

# Pré-processamento de Dados
data = data.drop(['Transaction ID', 'Date', 'Time', 'Shipping Address', 'Billing Address'], axis=1)

label_encoder = LabelEncoder()
categorical_cols = ['Card Type', 'Entry Mode', 'Transaction Type', 'Merchant Group', 'Transaction Country', 'Gender', 'Issuing Bank']

for col in categorical_cols:
    data[col] = label_encoder.fit_transform(data[col])

numeric_cols = ['Amount', 'Age']
scaler = StandardScaler()
data[numeric_cols] = scaler.fit_transform(data[numeric_cols])

# Convertendo os rótulos "No" e "Yes" para 0 e 1
data['Fraudulent'] = data['Fraudulent'].map({'No': 0, 'Yes': 1})

X = data.drop('Fraudulent', axis=1)
y = data['Fraudulent']

# Convertendo os dados para arrays NumPy
X = X.values
y = y.values

# Divisão dos dados
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Convertendo rótulos para o formato correto
y_train = np.asarray(y_train).astype('float32')
y_val = np.asarray(y_val).astype('float32')
y_test = np.asarray(y_test).astype('float32')

# Definindo a arquitetura da rede neural
model = Sequential()
model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compilando o modelo
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Treinando o modelo
history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))

# Avaliando o modelo no conjunto de teste
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"\nTest Accuracy: {test_accuracy*100:.2f}%")

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Carregando o conjunto de dados
data = pd.read_csv('/content/dataset.csv',delimiter=";")

# Pré-processamento de Dados
data = data.drop(['Transaction ID', 'Date', 'Time', 'Shipping Address', 'Billing Address'], axis=1)

label_encoder = LabelEncoder()
categorical_cols = ['Card Type', 'Entry Mode', 'Transaction Type', 'Merchant Group', 'Transaction Country', 'Gender', 'Issuing Bank']

for col in categorical_cols:
    data[col] = label_encoder.fit_transform(data[col])

numeric_cols = ['Amount', 'Age']
scaler = StandardScaler()
data[numeric_cols] = scaler.fit_transform(data[numeric_cols])

# Convertendo os rótulos "No" e "Yes" para 0 e 1
data['Fraudulent'] = data['Fraudulent'].map({'No': 0, 'Yes': 1})

X = data.drop('Fraudulent', axis=1)
y = data['Fraudulent']

# Convertendo os dados para arrays NumPy
X = X.values
y = y.values

# Divisão dos dados
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Convertendo rótulos para o formato correto
y_train = np.asarray(y_train).astype('float32')
y_val = np.asarray(y_val).astype('float32')
y_test = np.asarray(y_test).astype('float32')

# Definindo a arquitetura da rede neural
model = Sequential()
model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compilando o modelo
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Treinando o modelo
model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))

# Predições no conjunto de teste
y_pred = model.predict(X_test)
y_pred_binary = np.round(y_pred)

# Métricas de Avaliação
accuracy = accuracy_score(y_test, y_pred_binary)
precision = precision_score(y_test, y_pred_binary)
recall = recall_score(y_test, y_pred_binary)
f1 = f1_score(y_test, y_pred_binary)
roc_auc = roc_auc_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred_binary)

# Exibindo as métricas
print(f"Accuracy: {accuracy*100:.2f}%")
print(f"Precision: {precision*100:.2f}%")
print(f"Recall: {recall*100:.2f}%")
print(f"F1 Score: {f1*100:.2f}%")
print(f"AUC-ROC: {roc_auc*100:.2f}%")
print("\nConfusion Matrix:")
print(conf_matrix)

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Carregando o conjunto de dados
data = pd.read_csv('/content/dataset.csv',delimiter=";")

# Pré-processamento de Dados
data = data.drop(['Transaction ID', 'Date', 'Time', 'Shipping Address', 'Billing Address'], axis=1)

label_encoder = LabelEncoder()
categorical_cols = ['Card Type', 'Entry Mode', 'Transaction Type', 'Merchant Group', 'Transaction Country', 'Gender', 'Issuing Bank']

for col in categorical_cols:
    data[col] = label_encoder.fit_transform(data[col])

numeric_cols = ['Amount', 'Age']
scaler = StandardScaler()
data[numeric_cols] = scaler.fit_transform(data[numeric_cols])

# Convertendo os rótulos "No" e "Yes" para 0 e 1
data['Fraudulent'] = data['Fraudulent'].map({'No': 0, 'Yes': 1})

X = data.drop('Fraudulent', axis=1)
y = data['Fraudulent']

# Convertendo os dados para arrays NumPy
X = X.values
y = y.values

# Divisão dos dados
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Convertendo rótulos para o formato correto
y_train = np.asarray(y_train).astype('float32')
y_val = np.asarray(y_val).astype('float32')
y_test = np.asarray(y_test).astype('float32')

# Definindo a arquitetura da rede neural
model = Sequential()
model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compilando o modelo
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Treinando o modelo
model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))

# Predições no conjunto de teste
y_pred = model.predict(X_test)
y_pred_binary = np.round(y_pred)

# Métricas de Avaliação
accuracy = accuracy_score(y_test, y_pred_binary)
precision = precision_score(y_test, y_pred_binary)
recall = recall_score(y_test, y_pred_binary)
f1 = f1_score(y_test, y_pred_binary)
roc_auc = roc_auc_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred_binary)

# Relatório Técnico
report = f"""
Relatório Técnico - Detecção de Fraudes

**Processo de Modelagem:**
1. Carregamento e pré-processamento do conjunto de dados.
2. Escolha da arquitetura de rede neural profunda.
3. Treinamento do modelo com validação.
4. Avaliação do desempenho no conjunto de teste.

**Arquitetura da Rede Neural:**
- Camada de Entrada: 64 neurônios, ativação ReLU.
- Dropout de 50% para reduzir overfitting.
- Camada Oculta: 32 neurônios, ativação ReLU.
- Dropout de 50%.
- Camada de Saída: 1 neurônio, ativação sigmoid.

**Hiperparâmetros:**
- Otimizador: Adam.
- Função de Perda: Binary Crossentropy.
- Épocas: 10.
- Tamanho do Batch: 64.

**Resultados de Desempenho:**
- Acurácia: {accuracy*100:.2f}%
- Precisão: {precision*100:.2f}%
- Recall: {recall*100:.2f}%
- F1 Score: {f1*100:.2f}%
- AUC-ROC: {roc_auc*100:.2f}%
- Matriz de Confusão:
{conf_matrix}

**Observações Importantes:**
- O modelo demonstra um desempenho geral sólido, com alta precisão e recall.
- A área sob a curva ROC (AUC-ROC) é um indicativo robusto da capacidade do modelo em distinguir entre classes.

"""

print(report)